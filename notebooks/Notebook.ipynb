{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVCCkH4heCX1"
   },
   "source": [
    "# WMH Segmentation Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5vDrLXII1c4"
   },
   "source": [
    "A notebook that can build, train and run a model to tackle the White Matter Hyperintensities (WMH) segmentation challenge.\n",
    "\n",
    "A few options can be set to modify the method used (3D slices, preprocessed images...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axYPG6ITboZ3"
   },
   "source": [
    "## Colab integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_fb_MrLI1c4"
   },
   "source": [
    "A cell that can be run to easily run the notebook in Colab.\n",
    "\n",
    "With that, the notebook can be imported in Colab and the cell will download everything necessary to run it.\n",
    "\n",
    "Constants are defined at the start to be able to modify the execution (e.g. downloading another git branch than master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNNING_IN_COLAB = True\n",
    "\n",
    "if RUNNING_IN_COLAB:\n",
    "    REPO_URL = 'https://github.com/nicomem/imed-project.git'\n",
    "    BRANCH   = 'final-touches'\n",
    "    REPO_DIR = 'imed-project'\n",
    "    DATA_URL = 'https://drive.google.com/uc?id=1onHHWIhkhN5xYMit0rhhtVXlJrAlzCit'\n",
    "    \n",
    "    from pathlib import Path\n",
    "\n",
    "    %cd /content\n",
    "\n",
    "    # Download the repository\n",
    "    if not Path(REPO_DIR).is_dir():\n",
    "        !git clone --branch {BRANCH} --depth=1 -- {REPO_URL} {REPO_DIR}\n",
    "    \n",
    "    %cd {REPO_DIR}\n",
    "\n",
    "    # Install requirements\n",
    "    !pip install -r requirements.txt | grep -v 'Requirement already satisfied'\n",
    "    !pip install gdown | grep -v 'Requirement already satisfied'\n",
    "    \n",
    "    import gdown\n",
    "    if not Path('data.zip').is_file():\n",
    "        gdown.download(DATA_URL, 'data.zip', quiet=False)\n",
    "    \n",
    "    if not Path('data').is_dir():\n",
    "        !unzip -q -- data.zip\n",
    "    \n",
    "    %cd notebooks\n",
    "    %ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hu8S_JzboaB"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd-party imports\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_unet\n",
    "import skimage\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijoAUa1veCX_"
   },
   "source": [
    "## Get dataset & split train/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSq38BbWI1c4"
   },
   "source": [
    "Get the dataset files and split them into train/validation/test sets.\n",
    "\n",
    "The split is done by scan, so that different slices of the same scan will not be in multiple sets.\n",
    "\n",
    "The data is not loaded here, only the nibabel objects are created, which only loads information about the data (slices shape, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.load_data import *\n",
    "\n",
    "train_nib, val_nib, test_nib = get_dataset('../data', verbose=True)\n",
    "\n",
    "print('\\n{...}_nib keys:', train_nib.keys())\n",
    "print('train_nib:', [len(v) for v in train_nib.values()])\n",
    "print('val_nib:',   [len(v) for v in val_nib.values()])\n",
    "print('test_nib:',  [len(v) for v in test_nib.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Um7OQh3JeCYB"
   },
   "source": [
    "## Load train & analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNe6swFaI1c4"
   },
   "source": [
    "The train set data is loaded here.\n",
    "\n",
    "The `SlicesSequence` object herits the keras `Sequence` object which can be used to load lazily the data when fitting the model.\n",
    "\n",
    "However, if we have enough RAM, we can speed-up the data loading time by loading all slices at once, which is done here by the creation of `CachedSlicesSequence`.\n",
    "\n",
    "We also reshape the slices by cropping or padding them to have the same shape, so that they can be easily be transfered to the model (each slice in a batch must have the same shape)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control whether to use a 3D model and whether to add a preprocessing phase\n",
    "ENABLE_3D = False\n",
    "ENABLE_PREPROCESSING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On 3D, we must reduce the batch size or else we run out of GPU memory\n",
    "batch_size = 16 if ENABLE_3D else 32\n",
    "radius_3D = 1 if ENABLE_3D else 0\n",
    "preprocess = ENABLE_PREPROCESSING\n",
    "\n",
    "target_height = 256\n",
    "target_width = 256\n",
    "shuffle = True\n",
    "num_classes = 1\n",
    "\n",
    "img_size = (target_height, target_width)\n",
    "num_channels_per_slice = 2 + preprocess\n",
    "\n",
    "# (current slice + radius before + radius after) * number of channels per slice\n",
    "total_num_channels = (radius_3D * 2 + 1) * num_channels_per_slice\n",
    "\n",
    "slices_seq_kwargs = {\n",
    "    'target_height': target_height,\n",
    "    'target_width': target_width,\n",
    "    'slices3D_radius': radius_3D,\n",
    "    'batch_size': batch_size,\n",
    "    'shuffle': shuffle\n",
    "}\n",
    "\n",
    "# Create a lazy-loading sequence\n",
    "train_seq_uncached = SlicesSequence(train_nib, **slices_seq_kwargs)\n",
    "\n",
    "# Load all slices to speed up the training\n",
    "# Remove the slices where no wmh is found,\n",
    "# this leads to faster and more stable training\n",
    "train_seq = CachedSlicesSequence(train_seq_uncached,\n",
    "                                 preprocess=preprocess,\n",
    "                                 remove_no_wmh=True)\n",
    "\n",
    "print('Number of trainable slices:', len(train_seq.indexes))\n",
    "print('Number of batch:', len(train_seq))\n",
    "print('Batch size:', train_seq.batch_size)\n",
    "print('Slices not trained per epoch:', len(train_seq.indexes) - len(train_seq) * train_seq.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sequence can be accessed by batch like a list\n",
    "x,y = train_seq[0]\n",
    "print(x.dtype, y.dtype)\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "x,y = train_seq[-1]\n",
    "print(x.dtype, y.dtype)\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a random slice from 2 batchs\n",
    "i_data = 10\n",
    "\n",
    "x,y = train_seq[0]\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(x.shape[-1]):\n",
    "    plt.subplot(1, total_num_channels+1, i+1)\n",
    "    plt.imshow(x[i_data,...,i])\n",
    "plt.subplot(1, total_num_channels+1, total_num_channels+1)\n",
    "plt.imshow(y[i_data])\n",
    "plt.show()\n",
    "\n",
    "x,y = train_seq[-1]\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(total_num_channels):\n",
    "    plt.subplot(1, total_num_channels+1, i+1)\n",
    "    plt.imshow(x[i_data,...,i])\n",
    "plt.subplot(1, total_num_channels+1, total_num_channels+1)\n",
    "plt.imshow(y[i_data])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGCmLlkKeCYC"
   },
   "source": [
    "## Prepare the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xoEzdC7I1c4"
   },
   "source": [
    "The model chosen is a UNet, created with the help of a library to avoid all the boilerplate.\n",
    "\n",
    "It takes the T1 and FLAIR images of a slice and returns an image containing the probability of WMH for each pixel.\n",
    "\n",
    "The resulting probabilities can be transformed to boolean values by simply applying a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_unet.models import custom_unet\n",
    "\n",
    "input_shape = (target_height, target_width, total_num_channels)\n",
    "keras.backend.clear_session()\n",
    "model = custom_unet(\n",
    "    input_shape,\n",
    "    num_classes=1,\n",
    "    use_batch_norm=True,\n",
    "    filters=32,\n",
    "    num_layers=3,\n",
    "    dropout=0.1,\n",
    "    output_activation='sigmoid'\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJs74PmCeCYD"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdXJfvloI1c4"
   },
   "source": [
    "The validation set is loaded here in the same way as the training set.\n",
    "\n",
    "It is used to tweak hyper-parameters to improve metrics and decrease overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_seq_uncached = SlicesSequence(val_nib, **slices_seq_kwargs)\n",
    "\n",
    "# Do not remove the no-wmh slices because this is the validation set\n",
    "# Modifying this set would result in adding a bias to the metrics\n",
    "val_seq = CachedSlicesSequence(val_seq_uncached,\n",
    "                               preprocess=preprocess)\n",
    "len(val_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dice(keras.metrics.Metric):\n",
    "    def __init__(self, name='dice', threshold=0.5, **kwargs):\n",
    "        super(Dice, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.TP = keras.metrics.TruePositives(thresholds=threshold)\n",
    "        self.FP = keras.metrics.FalsePositives(thresholds=threshold)\n",
    "        self.TN = keras.metrics.TrueNegatives(thresholds=threshold)\n",
    "        self.FN = keras.metrics.FalseNegatives(thresholds=threshold)\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.TP.update_state(y_true, y_pred, sample_weight=sample_weight)\n",
    "        self.FP.update_state(y_true, y_pred, sample_weight=sample_weight)\n",
    "        self.TN.update_state(y_true, y_pred, sample_weight=sample_weight)\n",
    "        self.FN.update_state(y_true, y_pred, sample_weight=sample_weight)\n",
    "    \n",
    "    def result(self):\n",
    "        TP = self.TP.result()\n",
    "        FP = self.FP.result()\n",
    "        TN = self.TN.result()\n",
    "        FN = self.FN.result()\n",
    "\n",
    "        return 2 * TP / (2 * TP + FP + FN)\n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.TP.reset_states()\n",
    "        self.FP.reset_states()\n",
    "        self.TN.reset_states()\n",
    "        self.FN.reset_states()\n",
    "\n",
    "\n",
    "d = Dice()\n",
    "d.update_state([0,1,1,1], [0,0,0,1])\n",
    "print(d.result().numpy())\n",
    "\n",
    "d.update_state([0,1,1,1], [0.2,0.4,0.45,0.6])\n",
    "print(d.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\n",
    "        Dice(name='dice'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "        tf.keras.metrics.Precision(name='precision')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_filename = f\"model_radius-{radius_3D}_{'prepro' if preprocess else 'no-prepro'}.h5\"\n",
    "print(checkpoint_filename)\n",
    "callback_checkpoint = ModelCheckpoint(\n",
    "    checkpoint_filename, \n",
    "    verbose=1, \n",
    "    monitor='val_dice',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_seq,\n",
    "    epochs=50,\n",
    "    validation_data=val_seq,\n",
    "    shuffle=shuffle,\n",
    "    callbacks=[callback_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model checkpoint\n",
    "model.load_weights(checkpoint_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the learning curves with a rolling average (to make the plots easier to analyse)\n",
    "\n",
    "def rolling_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "ravg_w = 3\n",
    "\n",
    "plot_cols = ['loss', 'precision', 'recall', 'dice']\n",
    "plt.figure(figsize=(20,6))\n",
    "for i, col in enumerate(plot_cols):\n",
    "    plt.subplot(1, len(plot_cols), i+1)\n",
    "\n",
    "    plt.plot(rolling_average(history.history[col], ravg_w), label='train')\n",
    "    plt.plot(rolling_average(history.history[f'val_{col}'], ravg_w), label='val')\n",
    "\n",
    "    plt.title(col)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(col)\n",
    "    plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_with_set(model, seq, N = 10, bool_threshold = None):\n",
    "    '''\n",
    "    Display the model results on a random sample of data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model:\n",
    "        The trained model.\n",
    "    seq: SlicesSequence\n",
    "        The dataset to check.\n",
    "    N: int\n",
    "        The number of samples to check.\n",
    "    bool_threshold: Option[int]\n",
    "        The threshold to apply to the model results.\n",
    "        Must be in the range: [0.0, 1.0].\n",
    "        Set to None to display the raw results (probabilities).\n",
    "    '''\n",
    "    \n",
    "    # Pick a random sample of the dataset\n",
    "    i_samples = np.random.choice(np.arange(0, len(seq.X)), size=N, replace=False)\n",
    "    \n",
    "    # Predict the samples\n",
    "    X = seq.X[i_samples]\n",
    "    Y_gt = seq.Y[i_samples]\n",
    "    Y_pred = model.predict(X)\n",
    "\n",
    "    if bool_threshold:\n",
    "        Y_pred = (Y_pred > bool_threshold).astype(np.bool)\n",
    "\n",
    "    # Helper function to reshape the images, in case there are more single dimensions\n",
    "    reshape_img = lambda img: np.reshape(img, (target_height, target_width))\n",
    "        \n",
    "    # Compare some predictions to the ground truth\n",
    "    plt.figure(figsize=(20,5*N))\n",
    "    for i in range(N):\n",
    "        x_T1    = reshape_img(X[i,...,0])\n",
    "        x_FLAIR = reshape_img(X[i,...,1])\n",
    "        y_gt    = reshape_img(Y_gt[i])\n",
    "        y_pred  = reshape_img(Y_pred[i])\n",
    "\n",
    "        plt.subplot(N, 4, 4*i+1)\n",
    "        plt.imshow(x_T1)\n",
    "        plt.title('T1')\n",
    "\n",
    "        plt.subplot(N, 4, 4*i+2)\n",
    "        plt.imshow(x_FLAIR)\n",
    "        plt.title('FLAIR')\n",
    "\n",
    "        plt.subplot(N, 4, 4*i+3)\n",
    "        plt.imshow(y_gt)\n",
    "        plt.title('Ground Truth (wmh)')\n",
    "\n",
    "        plt.subplot(N, 4, 4*i+4)\n",
    "        plt.imshow(y_pred)\n",
    "        plt.title('Predicted (wmh)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_model_with_set(model, val_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bTgiK-fXetk"
   },
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xD1DnrDMI1c4"
   },
   "source": [
    "Evaluate the model on the testing set.\n",
    "\n",
    "This must be done after the model has \"good results\".\n",
    "\n",
    "**This must be used to tweak hyper-parameters** (or else, this defeats the goal of the testing set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the test set\n",
    "test_seq_uncached = SlicesSequence(test_nib, **slices_seq_kwargs)\n",
    "test_seq = CachedSlicesSequence(val_seq_uncached)\n",
    "print('Number of slices in test set:', len(test_seq) * batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_model_with_set(model, test_seq, N=20, bool_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
