{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVCCkH4heCX1"
   },
   "source": [
    "# 2D model without preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axYPG6ITboZ3"
   },
   "source": [
    "## Colab integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cell that can be run to easily run the notebook in Colab.\n",
    "\n",
    "With that, the notebook can be imported in Colab and the cell will download everything necessary to run it.\n",
    "\n",
    "Constants are defined at the start to be able to modify the execution (e.g. downloading another git branch than master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNNING_IN_COLAB = False\n",
    "\n",
    "if RUNNING_IN_COLAB:\n",
    "    REPO_URL = 'https://github.com/nicomem/imed-project.git'\n",
    "    BRANCH   = 'master'\n",
    "    REPO_DIR = 'imed-project'\n",
    "    DATA_URL = 'https://drive.google.com/uc?id=1onHHWIhkhN5xYMit0rhhtVXlJrAlzCit'\n",
    "    \n",
    "    from pathlib import Path\n",
    "\n",
    "    %cd /content\n",
    "\n",
    "    # Download the repository\n",
    "    if not Path(REPO_DIR).is_dir():\n",
    "        !git clone --branch {BRANCH} --depth=1 -- {REPO_URL} {REPO_DIR}\n",
    "    \n",
    "    %cd {REPO_DIR}\n",
    "\n",
    "    # Install requirements\n",
    "    !pip install -r requirements.txt | grep -v 'Requirement already satisfied'\n",
    "    !pip install gdown | grep -v 'Requirement already satisfied'\n",
    "    \n",
    "    import gdown\n",
    "    if not Path('data.zip').is_file():\n",
    "        gdown.download(DATA_URL, 'data.zip', quiet=False)\n",
    "    \n",
    "    if not Path('data').is_dir():\n",
    "        !unzip -q -- data.zip\n",
    "    \n",
    "    %cd notebooks\n",
    "    %ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hu8S_JzboaB"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd-party imports\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_unet\n",
    "import cv2\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijoAUa1veCX_"
   },
   "source": [
    "## Get dataset & split train/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the dataset files and split them into train/validation/test sets.\n",
    "\n",
    "The split is done by scan, so that different slices of the same scan will not be in multiple sets.\n",
    "\n",
    "The data is not loaded here, only the nibabel objects are created, which only loads information about the data (slices shape, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.load_data import get_dataset, SlicesSequence, CachedSlicesSequence\n",
    "\n",
    "train_nib, val_nib, test_nib = get_dataset('../data', verbose=True)\n",
    "\n",
    "print('\\n{...}_nib keys:', train_nib.keys())\n",
    "print('train_nib:', [len(v) for v in train_nib.values()])\n",
    "print('val_nib:',   [len(v) for v in val_nib.values()])\n",
    "print('test_nib:',  [len(v) for v in test_nib.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Um7OQh3JeCYB"
   },
   "source": [
    "## Load train & analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train set data is loaded here.\n",
    "\n",
    "The `SlicesSequence` object herits the keras `Sequence` object which can be used to load lazily the data when fitting the model.\n",
    "\n",
    "However, if we have enough RAM, we can speed-up the data loading time by loading all slices at once, which is done here by the creation of `CachedSlicesSequence`.\n",
    "\n",
    "We also reshape the slices by cropping or padding them to have the same shape, so that they can be easily be transfered to the model (each slice in a batch must have the same shape)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "shuffle = True\n",
    "target_height = 256\n",
    "target_width = 256\n",
    "img_size = (target_height, target_width)\n",
    "num_classes = 1\n",
    "\n",
    "train_seq_uncached = SlicesSequence(train_nib,\n",
    "                                    target_height=target_height,\n",
    "                                    target_width=target_width, \n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=shuffle)\n",
    "train_seq = CachedSlicesSequence(train_seq_uncached)\n",
    "\n",
    "print('Number of batch:', len(train_seq))\n",
    "print('Total number of slices:', len(train_seq.Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sequence can be accessed by batch like a list\n",
    "x,y = train_seq[0]\n",
    "print('Batch X:', x.shape)\n",
    "print('Batch Y:', y.shape)\n",
    "print('A slice (T1, FLAIR, wmh):',\n",
    "      x[0,...,0].shape, x[0,...,1].shape, y[0].shape)\n",
    "\n",
    "print('---')\n",
    "\n",
    "x,y = train_seq[-1]\n",
    "print('Batch X:', x.shape)\n",
    "print('Batch Y:', y.shape)\n",
    "print('A slice (T1, FLAIR, wmh):',\n",
    "      x[0,...,0].shape, x[0,...,1].shape, y[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a random slice from 2 batchs\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "i_data = 15\n",
    "\n",
    "x,y = train_seq[0]\n",
    "plt.subplot(2, 3, 1); plt.imshow(x[i_data,...,0])\n",
    "plt.subplot(2, 3, 2); plt.imshow(x[i_data,...,1])\n",
    "plt.subplot(2, 3, 3); plt.imshow(y[i_data])\n",
    "\n",
    "x,y = train_seq[-1]\n",
    "plt.subplot(2, 3, 4); plt.imshow(x[i_data,...,0])\n",
    "plt.subplot(2, 3, 5); plt.imshow(x[i_data,...,1])\n",
    "plt.subplot(2, 3, 6); plt.imshow(y[i_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGCmLlkKeCYC"
   },
   "source": [
    "## Prepare the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model chosen is a UNet, created with the help of a library to avoid all the boilerplate.\n",
    "\n",
    "It takes the T1 and FLAIR images of a slice and returns an image containing the probability of WMH for each pixel.\n",
    "\n",
    "The resulting probabilities can be transformed to boolean values by simply applying a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_unet.models import custom_unet\n",
    "\n",
    "input_shape = (target_height, target_width, 2)\n",
    "keras.backend.clear_session()\n",
    "model = custom_unet(\n",
    "    input_shape,\n",
    "    num_classes=1,\n",
    "    use_batch_norm=True,\n",
    "    filters=32,\n",
    "    num_layers=3,\n",
    "    dropout=0.1,\n",
    "    output_activation='sigmoid'\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJs74PmCeCYD"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation set is loaded here in the same way as the training set.\n",
    "\n",
    "It is used to tweak hyper-parameters to improve metrics and decrease overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_seq_uncached = SlicesSequence(val_nib,\n",
    "                                  target_height=target_height,\n",
    "                                  target_width=target_width, \n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=shuffle)\n",
    "val_seq = CachedSlicesSequence(val_seq_uncached)\n",
    "len(val_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a lot of slices do not contain any WMH, only those that contains some are kept for training.\n",
    "\n",
    "This should result in a faster training and should not worsen the model results since the WMH areas are not located at the same place in every slice.\n",
    "\n",
    "However, all slices in the validation and testing sets are kept, to avoid adding a bias to the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usable_train_data = np.any(train_seq.Y, axis=(1,2))\n",
    "\n",
    "nb_wmh_slices = np.count_nonzero(usable_train_data)\n",
    "\n",
    "print('Number of WMH-present training slices:', nb_wmh_slices)\n",
    "print('Number of original    training slices:', len(train_seq.Y))\n",
    "\n",
    "print(f'\\n{100 * nb_wmh_slices / len(train_seq.Y):.2f}%',\n",
    "      'of the training slices have been kept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_wmh_slices_val = np.count_nonzero(np.any(val_seq.Y, axis=(1,2)))\n",
    "\n",
    "print(f'{100 * nb_wmh_slices_val / val_seq.Y.shape[0]:.2f}%',\n",
    "      'of validation slices contains WMH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_filename = 'segm_model_v0.h5'\n",
    "callback_checkpoint = ModelCheckpoint(\n",
    "    checkpoint_filename, \n",
    "    verbose=1, \n",
    "    monitor='val_loss', \n",
    "    save_best_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[tf.keras.metrics.Recall(name='recall'),\n",
    "             tf.keras.metrics.Precision(name='precision')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_seq.X[usable_train_data],\n",
    "    train_seq.Y[usable_train_data],\n",
    "    epochs=40,\n",
    "    validation_data=(val_seq.X, val_seq.Y),\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    callbacks=[callback_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model checkpoint\n",
    "model.load_weights(checkpoint_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the learning curves with a rolling average (to make the plots easier to analyse)\n",
    "\n",
    "def rolling_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "ravg_w = 3\n",
    "\n",
    "plot_cols = ['loss', 'precision', 'recall']\n",
    "plt.figure(figsize=(20,6))\n",
    "for i, col in enumerate(plot_cols):\n",
    "    plt.subplot(1, len(plot_cols), i+1)\n",
    "\n",
    "    plt.plot(rolling_average(history.history[col], ravg_w), label='train')\n",
    "    plt.plot(rolling_average(history.history[f'val_{col}'], ravg_w), label='val')\n",
    "\n",
    "    plt.title(col)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(col)\n",
    "    plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_with_set(model, seq, N = 10, bool_threshold = None):\n",
    "    '''\n",
    "    Display the model results on a random sample of data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model:\n",
    "        The trained model.\n",
    "    seq: SlicesSequence\n",
    "        The dataset to check.\n",
    "    N: int\n",
    "        The number of samples to check.\n",
    "    bool_threshold: Option[int]\n",
    "        The threshold to apply to the model results.\n",
    "        Must be in the range: [0.0, 1.0].\n",
    "        Set to None to display the raw results (probabilities).\n",
    "    '''\n",
    "    \n",
    "    # Pick a random sample of the dataset\n",
    "    i_samples = np.random.choice(np.arange(0, len(seq.X)), size=N, replace=False)\n",
    "    \n",
    "    # Predict the samples\n",
    "    X = seq.X[i_samples]\n",
    "    Y_gt = seq.Y[i_samples]\n",
    "    Y_pred = model.predict(X)\n",
    "\n",
    "    if bool_threshold:\n",
    "        Y_pred = (Y_pred > bool_threshold).astype(np.bool)\n",
    "\n",
    "    # Helper function to reshape the images, in case there are more single dimensions\n",
    "    reshape_img = lambda img: np.reshape(img, (target_height, target_width))\n",
    "        \n",
    "    # Compare some predictions to the ground truth\n",
    "    plt.figure(figsize=(20,5*N))\n",
    "    for i in range(N):\n",
    "        x_T1    = reshape_img(X[i,...,0])\n",
    "        x_FLAIR = reshape_img(X[i,...,1])\n",
    "        y_gt    = reshape_img(Y_gt[i])\n",
    "        y_pred  = reshape_img(Y_pred[i])\n",
    "\n",
    "        plt.subplot(N, 4, 4*i+1)\n",
    "        plt.imshow(x_T1)\n",
    "        plt.title('T1')\n",
    "\n",
    "        plt.subplot(N, 4, 4*i+2)\n",
    "        plt.imshow(x_FLAIR)\n",
    "        plt.title('FLAIR')\n",
    "\n",
    "        plt.subplot(N, 4, 4*i+3)\n",
    "        plt.imshow(y_gt)\n",
    "        plt.title('Ground Truth (wmh)')\n",
    "\n",
    "        plt.subplot(N, 4, 4*i+4)\n",
    "        plt.imshow(y_pred)\n",
    "        plt.title('Predicted (wmh)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_model_with_set(model, val_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bTgiK-fXetk"
   },
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on the testing set.\n",
    "\n",
    "This must be done after the model has \"good results\".\n",
    "\n",
    "**This must be used to tweak hyper-parameters** (or else, this defeats the goal of the testing set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the test set\n",
    "test_seq_uncached = SlicesSequence(test_nib,\n",
    "                                   target_height=target_height,\n",
    "                                   target_width=target_width, \n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=shuffle)\n",
    "test_seq = CachedSlicesSequence(val_seq_uncached)\n",
    "print('Number of slices in test set:', len(test_seq) * batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test_seq.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = keras.metrics.Precision()\n",
    "precision.update_state(test_seq.Y, test_pred)\n",
    "print(f'Test Precision: {100 * precision.result().numpy():.2f}%')\n",
    "\n",
    "recall = keras.metrics.Recall()\n",
    "recall.update_state(test_seq.Y, test_pred)\n",
    "print(f'Test Recall   : {100 * recall.result().numpy():.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_model_with_set(model, test_seq, N=20, bool_threshold=0.5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
